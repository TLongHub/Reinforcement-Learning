import torch as t

#   INITIALISING
K = 10 # k is number of actions at each state
# Actions run from 0 to 9
Q = t.zeros(1, K)
N = 0

def initialise():
    global Q, N, K
    Q = t.zeros(1, K) # Initiate Q as 0 everywhere
    N = 0 # Initialise N, the time step

#   GENERATING ACTION VALUES - q^*(a)
q_star = t.normal(0, 1, size=(1, K)) # Vector of action values, q^*(a) 

#   DEFINE FUNCTION TO COMPUTE ACTUAL REWARD
def actual_reward(q_star, action):
    reward = t.normal(t.select(q_star, 1, action), 1) # Takes q^*(a) as mean and 1 as variance
    return reward


## UPDATE METHODS

#   SAMPLE AVERAGE METHOD - estimate q^*(a) by taking sample average from experience, call the estimate Q(a)

def SA_update(action, reward):
    global Q, N
    """Updates the estimates Q according to experienced rewards using sample average."""
    mu = t.select(Q, 1, action) # Q(a)
    Q[0, action] = t.tensor([(mu * N + reward)/(N + 1)]) # Update the Q(a) value
    N =+ 1 # Update n

#   DISCOUNTED REWARD FOR NON-STATIONARY BANDIT PROBLEMS - discounting factor, alpha,
#  means we favour more recent rewards more than older rewards. 

def DF_update(action, reward, alpha):
    global Q, N
    """Updates the estimates Q according to experienced rewards using discounting factor, alpha."""
    mu = t.select(Q, 1, action) # Q(a)
    Q[0, action] = t.tensor([mu + alpha * (reward - mu)]) # Update the Q(a) value
    N =+ 1 # Update n


## LEARNING ALGORITHMS

#   GREEDY APPROACH - first 10 goes, we try each action once, afterwards, we use best Q(a)
# can also do first 20 or 30 for 2 or 3 attempts at each action

def greedy(q_star, alpha, DF = False):
    global Q, N, K
    """Performs greedy learning algorithm for k-bandit problem."""
    tc = 0 # Number of exploration cycles (select 0 for no exploration)
    n = 500 # Number of additional time steps
    for _ in range(tc): # Start exploration cycle
        for action in range(K): # Itterate over all possible actions
            reward = actual_reward(q_star, action)
            if DF == False:
                SA_update(action, reward)
            else:
                DF_update(action, reward, alpha)
    
    for _ in range(n): # Start exploitation cycles
        action = t.argmax(Q) # Selecet the most favoured action
        reward = actual_reward(q_star, action)
        if DF == False:
            SA_update(action, reward)
        else:
            DF_update(action, reward, alpha)
    print("The last action selected (favoured):", action)

#   EPSILON-GREEDY APPROACH - greedy approach but w.p. epsilon, choose a random action

def epsilon_greedy(q_star, alpha, epsilon, DF = False): # Very similar to greedy approach...
    global Q, N, K
    """Performs epsilon-greedy learning algorithm for k-bandit problem."""
    n = 500 # Number of additional time steps
    
    for _ in range(n):
        if t.rand(1) <= epsilon: # With probability epsilon, choose action at random
            action = t.randint(10, (1, 1))
        else:
            action = t.argmax(Q) # Selecet the most favoured action
        reward = actual_reward(q_star, action)
        if DF == False:
            SA_update(action, reward)
        else:
            DF_update(action, reward, alpha)
    print("The last action selected (favoured):", action)


#   TESTING 
# GREEDY with/without discounting
alpha = 0.1

initialise()
greedy(q_star, alpha, True) # With DF
#print("q estimates:\n", Q)
print("Optimal action was:", t.argmax(q_star))

initialise()
greedy(q_star, alpha) # Without DF
#print("q estimates:\n", Q)
print("Optimal action was:", t.argmax(q_star))


# EPSILON-GREEDY with/without discounting
epsilon = 0.1

initialise()
epsilon_greedy(q_star, alpha, epsilon, True) # With DF
#print("q estimates:\n", Q)
print("Optimal action was:", t.argmax(q_star))

initialise()
epsilon_greedy(q_star, alpha, epsilon) # Without DF
#print("q estimates:\n", Q)
print("Optimal action was:", t.argmax(q_star))


#   BULK TESTING

def test_V1(num_tests):
    """Complete a number of runs of greedy and epsilon-greedy algorithms and track their success."""
    global Q, N, K
    for _ in range(num_tests):
        q_star = t.normal(0, 1, size=(1, K)) #Set random action values
        initialise() #Initialise estimates, Q, and time steps, N
        
