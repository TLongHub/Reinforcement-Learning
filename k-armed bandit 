import torch as t
import matplotlib.pyplot as plt

#   INITIALISING
K = 10 # k is number of actions at each state
# Actions run from 0 to 9
Q = t.zeros(1, K)
N = 0

def initialise():
    global Q, N, K
    Q = t.zeros(1, K) # Initiate Q as 0 everywhere
    N = 0 # Initialise N, the time step

#   GENERATING ACTION VALUES - q^*(a)
q_star = t.normal(0, 1, size=(1, K)) # Vector of action values, q^*(a) 

#   DEFINE FUNCTION TO COMPUTE ACTUAL REWARD
def actual_reward(q_star, action):
    reward = t.normal(t.select(q_star, 1, action), 1) # Takes q^*(a) as mean and 1 as variance
    return reward


## UPDATE METHODS

#   SAMPLE AVERAGE METHOD - estimate q^*(a) by taking sample average from experience, call the estimate Q(a)

def SA_update(action, reward):
    global Q, N
    """Updates the estimates Q according to experienced rewards using sample average."""
    mu = t.select(Q, 1, action) # Q(a)
    Q[0, action] = t.tensor([(mu * N + reward)/(N + 1)]) # Update the Q(a) value
    N =+ 1 # Update n

#   DISCOUNTED REWARD FOR NON-STATIONARY BANDIT PROBLEMS - discounting factor, alpha,
#  means we favour more recent rewards more than older rewards. 

def DF_update(action, reward, alpha):
    global Q, N
    """Updates the estimates Q according to experienced rewards using discounting factor, alpha."""
    mu = t.select(Q, 1, action) # Q(a)
    Q[0, action] = t.tensor([mu + alpha * (reward - mu)]) # Update the Q(a) value
    N =+ 1 # Update n


## LEARNING ALGORITHMS

#   GREEDY APPROACH - first 10 goes, we try each action once, afterwards, we use best Q(a)
# can also do first 20 or 30 for 2 or 3 attempts at each action

def greedy(q_star, alpha, DF = False):
    global Q, N, K
    """Performs greedy learning algorithm for k-bandit problem."""
    tc = 0 # Number of exploration cycles (select 0 for no exploration)
    n = 1000 # Number of additional time steps
    optimals = 0
    optimal_props = []
    for _ in range(tc): # Start exploration cycle
        for action in range(K): # Itterate over all possible actions

            reward = actual_reward(q_star, action)
            if DF == False:
                SA_update(action, reward)
            else:
                DF_update(action, reward, alpha)

            #OPTIMAL ACTION PROPORTION
            if action == t.argmax(q_star): #Keep track of number of optimal actions taken
                optimals += 1
            optimal_props.append(optimals)
            #OPTIMAL ACTION PROPORTION
    
    for _ in range(n): # Start exploitation cycles
        action = t.argmax(Q) # Selecet the most favoured action

        reward = actual_reward(q_star, action)
        if DF == False:
            SA_update(action, reward)
        else:
            DF_update(action, reward, alpha)

        #OPTIMAL ACTION PROPORTION
        if action == t.argmax(q_star): #Keep track of number of optimal actions taken
            optimals += 1
        optimal_props.append(optimals)
        #OPTIMAL ACTION PROPORTION

    return optimal_props

#   EPSILON-GREEDY APPROACH - greedy approach but w.p. epsilon, choose a random action

def epsilon_greedy(q_star, alpha, epsilon, DF = False): # Very similar to greedy approach...
    global Q, N, K
    """Performs epsilon-greedy learning algorithm for k-bandit problem."""
    n = 1000 # Number of additional time steps
    optimals = 0
    optimal_props = []
    for _ in range(n):
        if t.rand(1) <= epsilon: # With probability epsilon, choose action at random
            action = t.randint(10, (1, 1))
        else:
            action = t.argmax(Q) # Selecet the most favoured action

        reward = actual_reward(q_star, action)
        if DF == False:
            SA_update(action, reward)
        else:
            DF_update(action, reward, alpha)

        #OPTIMAL ACTION PROPORTION
        if action == t.argmax(q_star): #Keep track of number of optimal actions taken
            optimals += 1
        optimal_props.append(optimals)
        #OPTIMAL ACTION PROPORTION

    return optimal_props


#   BULK TESTING

def test_V1(num_tests):
    """Complete a number of runs of greedy and epsilon-greedy algorithms and track their success."""
    global Q, N, K
    prop_matrix = t.zeros(num_tests, 1000) #Create matrix of N x N size where N is number of steps in algorithm
    for i in range(num_tests):
        q_star = t.normal(0, 3, size=(1, K)) #Set random action values
        initialise() #Initialise estimates, Q, and time steps, N
        props = epsilon_greedy(q_star, 0, 0, False)
        prop_matrix[i, :] = t.tensor(props) #Add the props to matrix
    
    optimal_proportions = []
    for j in range(1000):
        prop_j = sum(prop_matrix[:, j]) / (num_tests*j) #The proportion of optimal actions taken by time step j
        optimal_proportions.append(prop_j)

    time_steps = [step for step in (range(1000))]

    plt.plot(time_steps, optimal_proportions, color = 'blue')

    prop_matrix = t.zeros(num_tests, 1000) #Create matrix of N x N size where N is number of steps in algorithm
    for i in range(num_tests):
        q_star = t.normal(0, 3, size=(1, K)) #Set random action values
        initialise() #Initialise estimates, Q, and time steps, N
        props = epsilon_greedy(q_star, 0, 0.1, False)
        prop_matrix[i, :] = t.tensor(props) #Add the props to matrix
    
    optimal_proportions = []
    for j in range(1000):
        prop_j = sum(prop_matrix[:, j]) / (num_tests*j) #The proportion of optimal actions taken by time step j
        optimal_proportions.append(prop_j)

    time_steps = [step for step in (range(1000))]

    plt.plot(time_steps, optimal_proportions, color = 'red')
    plt.show()

test_V1(1000)


