import torch as t

#   INITIALISING
K = 10 # k is number of actions at each state
# Actions run from 0 to 9
Q = t.zeros(1, K)
N = 0

#   GENERATING ACTION VALUES - q^*(a)
q_star = t.normal(0, 1, size=(1, K)) # Vector of action values, q^*(a) 

#   DEFINE FUNCTION TO COMPUTE ACTUAL REWARD
def actual_reward(q_star, action):
    reward = t.normal(t.select(q_star, 1, action), 1) # Takes q^*(a) as mean and 1 as variance
    return reward

#   SAMPLE AVERAGE METHOD - estimate q^*(a) by taking sample average from experience, call the estimate Q(a)
def initialise():
    global Q, N, K
    Q = t.zeros(1, K) # Initiate Q as 0 everywhere
    N = 0 # Initialise N, the time step

def SA_update(action, reward):
    global Q, N
    """Updates the estimates Q according to experienced rewards using sample average."""
    mu = t.select(Q, 1, action) # Q(a)
    Q[0, action] = t.tensor([(mu * N + reward)/(N + 1)]) # Update the Q(a) value
    N =+ 1 # Update n

#   GREEDY APPROACH - first 10 goes, we try each action once, afterwards, we use best Q(a)
# can also do first 20 or 30 for 2 or 3 attempts at each action

def greedy(q_star):
    global Q, N, K
    """Performs greedy learning algorithm for k-bandit problem."""
    tc = 0 # Number of exploration cycles (select 0 for no exploration)
    n = 500 # Number of additional time steps
    for _ in range(tc): # Start exploration cycle
        for action in range(K): # Itterate over all possible actions
            reward = actual_reward(q_star, action)
            SA_update(action, reward)
    
    for _ in range(n): # Start exploitation cycles
        action = t.argmax(Q) # Selecet the most favoured action
        reward = actual_reward(q_star, action)
        SA_update(action, reward)
    print("The last action selected (favoured):", action)

initialise()
greedy(q_star)
#print("q estimates:\n", Q)
print("Optimal action was:", t.argmax(q_star))

#   EPSILON-GREEDY APPROACH - greedy approach but w.p. epsilon, choose a random action

def epsilon_greedy(q_star, epsilon): # Very similar to greedy approach...
    global Q, N, K
    """Performs epsilon-greedy learning algorithm for k-bandit problem."""
    n = 500 # Number of additional time steps
    
    for _ in range(n):
        if t.rand(1) <= epsilon: # With probability epsilon, choose action at random
            action = t.randint(10, (1, 1))
        else:
            action = t.argmax(Q) # Selecet the most favoured action
        reward = actual_reward(q_star, action)
        SA_update(action, reward)
    print("The last action selected (favoured):", action)

initialise()
epsilon_greedy(q_star, 0.01)
#print("q estimates:\n", Q)
print("Optimal action was:", t.argmax(q_star))

#   DISCOUNTED REWARD FOR NON-STATIONARY BANDIT PROBLEMS - discounting factor, alpha,
#  means we favour more recent rewards more than older rewards. 

def DF_update(action, reward, alpha):
    global Q, N
    """Updates the estimates Q according to experienced rewards using discounting factor, alpha."""
    mu = t.select(Q, 1, action) # Q(a)
    Q[0, action] = t.tensor([mu + alpha * (reward - mu)]) # Update the Q(a) value
    N =+ 1 # Update n

